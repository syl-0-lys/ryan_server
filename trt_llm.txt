  TensorRT-LLM Server 调用链深度分析

  一、纯文本模型（DeepSeek-R1）调用链

  完整调用流程图

  用户请求 (POST /v1/chat/completions)
      ↓
  FastAPI 路由 (openai_server.py:256)
      ↓
  【入口】OpenAIServer.openai_chat(request, raw_request)
      ├─ 步骤 1: 解析消息（无多模态）
      │   └─ parse_chat_messages_coroutines(request.messages)
      │       └─ chat_utils.py:223-249
      │           ├─ 创建 MultimodalDataTracker（但无数据）
      │           ├─ parse_chat_message_content() - 仅解析文本
      │           └─ 返回: (conversation, None, [])
      │                    无 mm_coroutines ────┐
      ├─ 步骤 2: 应用聊天模板                    │
      │   └─ apply_chat_template()              │
      │       └─ inputs/utils.py:564            │
      │           ├─ 解析 tokenizer             │
      │           └─ tokenizer.apply_chat_template()
      │               → 返回纯文本字符串         │
      │                                         │
      ├─ 步骤 3: 转换为输入格式                  │
      │   └─ prompt = prompt_inputs(prompt)     │
      │       └─ TextPrompt(prompt=str)         │
      │           → 仅包含 "prompt" 字段         │
      │                                         │
      ├─ 步骤 4: 等待多模态数据（跳过）←─────────┘
      │   └─ mm_data = await mm_coroutines
      │       → mm_data = None（无多模态数据）
      │   └─ 不添加 multi_modal_data 字段
      │
      ├─ 步骤 5: 创建采样参数
      │   └─ sampling_params = request.to_sampling_params()
      │       └─ SamplingParams(temperature, top_p, ...)
      │
      ├─ 步骤 6: 提交推理请求
      │   └─ promise = self.llm.generate_async(
      │           inputs=prompt,              # TextPrompt
      │           sampling_params=sampling_params,
      │           streaming=request.stream
      │       )
      │       ↓
      │   【LLM 层】llm.py:generate_async
      │       ├─ 输入处理: input_processor(prompt, sampling_params)
      │       │   └─ 返回: (prompt_token_ids, None)  # 无额外处理
      │       ├─ 提交到 Executor
      │       │   └─ GenerationExecutor.submit()
      │       │       └─ TensorRT Engine 推理
      │       └─ 返回 RequestOutput promise
      │
      └─ 步骤 7: 返回响应
          ├─ 流式: StreamingResponse(chat_stream_generator())
          └─ 非流式: JSONResponse(ChatCompletionResponse)

  关键数据结构（纯文本）

  # 输入格式
  TextPrompt = {
      "prompt": "User: 你好\nAssistant: "
      # 无 multi_modal_data 字段
  }

  # 处理器返回
  (prompt_token_ids=[1, 2, 3, ...], extra_processed_inputs=None)

  # MultimodalDataTracker 状态
  _data = {}  # 空字典
  _placeholder_counts = {}  # 空字典

  ---

● 二、多模态模型（DeepSeek-VL）调用链

  完整调用流程图

  用户请求 (POST /v1/chat/completions)
  {
    "messages": [
      {
        "role": "user",
        "content": [
          {"type": "text", "text": "描述这张图"},
          {"type": "image_url", "image_url": {"url": "https://..."}}
        ]
      }
    ]
  }
      ↓
  FastAPI 路由 (openai_server.py:256)
      ↓
  【入口】OpenAIServer.openai_chat(request, raw_request)
      │
      ├─ 步骤 1: 解析多模态消息 ⭐
      │   └─ parse_chat_messages_coroutines(request.messages, model_config, multimodal_server_config)
      │       └─ chat_utils.py:223-249
      │           ├─ 创建 MultimodalDataTracker(model_type="deepseek_vl")
      │           │   └─ inputs/utils.py:438-449
      │           │
      │           ├─ 遍历每条消息
      │           │   └─ parse_chat_message_content(msg, mm_data_tracker)
      │           │       └─ chat_utils.py:171-194
      │           │           └─ parse_chat_message_content_parts(role, content_parts)
      │           │               └─ chat_utils.py:148-168
      │           │                   ├─ 处理文本部分
      │           │                   │   → text_parts.append("描述这张图")
      │           │                   │
      │           │                   └─ 处理图像部分 ⭐
      │           │                       └─ parse_chat_message_content_part(part, mm_data_tracker)
      │           │                           └─ chat_utils.py:80-145
      │           │                               ├─ 识别 part_type = "image_url"
      │           │                               │
      │           │                               └─ 创建异步加载器
      │           │                                   ```python
      │           │                                   async def load_image_async():
      │           │                                       image_kwargs = multimodal_server_config.media_io_kwargs.get("image", {})
      │           │                                       return await async_load_image(url, **image_kwargs)
      │           │                                   ```
      │           │                                   └─ 返回 MultimodalData(
      │           │                                           modality="image",
      │           │                                           data=load_image_async()  # Coroutine对象
      │           │                                       )
      │           │
      │           ├─ 添加多模态数据到追踪器 ⭐
      │           │   └─ mm_data_tracker.add_data("image", load_image_async())
      │           │       └─ inputs/utils.py:468-474
      │           │           ├─ current_count = 1
      │           │           ├─ placeholder = retrieve_multimodal_placeholder("deepseek_vl", "image", 1)
      │           │           │   └─ 返回 "<image>"
      │           │           ├─ _data["image"].append(load_image_async())
      │           │           └─ _placeholder_counts["<image>"] = 1
      │           │
      │           ├─ 添加占位符到文本 ⭐
      │           │   └─ add_multimodal_placeholders(model_type, content, mm_placeholder_count)
      │           │       └─ inputs/utils.py:481-499
      │           │           ├─ placeholders = ["<image>"]
      │           │           └─ 根据模型类型决定位置
      │           │               → 返回: "<image>描述这张图"  或  "描述这张图<image>"
      │           │
      │           └─ 返回三元组 ⭐
      │               ├─ conversation: [{"role": "user", "content": "<image>描述这张图", "media": [...]}]
      │               ├─ mm_coroutines: mm_data_tracker.retrieve_all_async()  # 异步协程
      │               └─ mm_placeholder_counts: [{"<image>": 1}]
      │
      ├─ 步骤 2: 应用聊天模板（包含占位符）
      │   └─ prompt = apply_chat_template(
      │           model_type="deepseek_vl",
      │           conversation=conversation,  # 包含 "<image>" 占位符
      │           mm_placeholder_counts=mm_placeholder_counts
      │       )
      │       └─ inputs/utils.py:564-603
      │           ├─ 检查模型是否在 PLACEHOLDER_EXCEPTIONS
      │           │   └─ DeepSeek-VL 不在列表中，使用标准流程
      │           │
      │           └─ tokenizer.apply_chat_template()
      │               → 返回: "User: <image>描述这张图\nAssistant: "
      │
      ├─ 步骤 3: 转换为输入格式
      │   └─ prompt = prompt_inputs(prompt)
      │       → TextPrompt(prompt="User: <image>描述这张图\nAssistant: ")
      │
      ├─ 步骤 4: 等待多模态数据加载 ⭐⭐⭐
      │   └─ mm_data = await mm_coroutines
      │       └─ mm_data_tracker.retrieve_all_async()
      │           └─ inputs/utils.py:451-459
      │               ```python
      │               return {
      │                   modality: await asyncio.gather(*items)
      │                   for modality, items in self._data.items()
      │               }
      │               ```
      │               ├─ 执行 await asyncio.gather(*[load_image_async()])
      │               │   └─ async_load_image(url, format="pt", device="cpu")
      │               │       └─ inputs/utils.py:140-164
      │               │           ├─ 异步下载图像（如果是 URL）
      │               │           │   └─ aiohttp.ClientSession.get(url)
      │               │           ├─ PIL.Image 解码
      │               │           └─ 转换为 PyTorch Tensor
      │               │               → torch.Tensor[3, H, W]
      │               │
      │               └─ 返回: {"image": [torch.Tensor]}
      │
      │   └─ 将多模态数据添加到输入 ⭐
      │       ```python
      │       if mm_data is not None:
      │           prompt["multi_modal_data"] = mm_data
      │       ```
      │       → 现在 prompt = {
      │             "prompt": "User: <image>描述这张图\nAssistant: ",
      │             "multi_modal_data": {
      │                 "image": [torch.Tensor[3, 384, 384]]
      │             }
      │         }
      │
      ├─ 步骤 5: 创建采样参数（与纯文本相同）
      │   └─ sampling_params = request.to_sampling_params()
      │
      ├─ 步骤 6: 提交推理请求 ⭐
      │   └─ promise = self.llm.generate_async(
      │           inputs=prompt,              # TextPrompt + multi_modal_data
      │           sampling_params=sampling_params,
      │           streaming=request.stream
      │       )
      │       ↓
      │   【LLM 层】llm.py:generate_async
      │       ├─ 检测到 multi_modal_data 字段 ⭐
      │       │
      │       ├─ 输入处理: input_processor(prompt, sampling_params)
      │       │   └─ BaseMultimodalInputProcessor.__call__()
      │       │       └─ inputs/registry.py
      │       │           ├─ 提取文本: "User: <image>描述这张图\nAssistant: "
      │       │           │
      │       │           ├─ 提取多模态数据: {"image": [torch.Tensor]}
      │       │           │
      │       │           ├─ 调用模型特定处理器 ⭐⭐
      │       │           │   └─ DeepSeekVLInputProcessor (假设)
      │       │           │       ├─ 处理图像张量
      │       │           │       │   └─ 视觉编码器预处理
      │       │           │       │       ├─ resize 到 384x384
      │       │           │       │       ├─ normalize
      │       │           │       │       └─ 返回 pixel_values
      │       │           │       │
      │       │           │       ├─ 定位 <image> 占位符在 token 序列中的位置
      │       │           │       │   └─ tokenize("User: <image>描述这张图\nAssistant: ")
      │       │           │       │       → [1, 234, 100015, 5678, ...]
      │       │           │       │           ↑ <image> token ID
      │       │           │       │
      │       │           │       └─ 创建 MultimodalParams
      │       │           │           └─ inputs/multimodal.py
      │       │           │               ```python
      │       │           │               MultimodalParams(
      │       │           │                   multimodal_input=MultimodalInput(
      │       │           │                       multimodal_hashes=[[hash_values]],
      │       │           │                       multimodal_positions=[position_of_image_token],
      │       │           │                       multimodal_lengths=[576]  # DeepSeek-VL 图像占576 tokens
      │       │           │                   ),
      │       │           │                   multimodal_data={
      │       │           │                       "image": {
      │       │           │                           "pixel_values": torch.Tensor,
      │       │           │                           "image_height": 384,
      │       │           │                           "image_width": 384
      │       │           │                       }
      │       │           │                   }
      │       │           │               )
      │       │           │               ```
      │       │           │
      │       │           └─ 返回: (
      │       │                   prompt_token_ids=[1, 234, 100015, 5678, ...],
      │       │                   extra_processed_inputs={
      │       │                       "multimodal_params": MultimodalParams(...)
      │       │                   }
      │       │               )
      │       │
      │       ├─ 提交到 Executor ⭐
      │       │   └─ GenerationExecutor.submit(
      │       │           prompt_token_ids,
      │       │           multimodal_params=extra_processed_inputs["multimodal_params"]
      │       │       )
      │       │       ↓
      │       │   【Executor 层】
      │       │       ├─ 检测到 multimodal_params
      │       │       │
      │       │       └─ TensorRT Engine 多模态推理
      │       │           ├─ 视觉编码器前向传播
      │       │           │   └─ 将 pixel_values 编码为 vision_embeddings [576, 2048]
      │       │           │
      │       │           ├─ 文本 Embedding
      │       │           │   └─ 将 prompt_token_ids 转换为 text_embeddings
      │       │           │
      │       │           ├─ 在 <image> token 位置插入 vision_embeddings ⭐⭐⭐
      │       │           │   ```
      │       │           │   最终序列:
      │       │           │   [text_emb[0], text_emb[1], vision_emb[0:576], text_emb[3], ...]
      │       │           │   ```
      │       │           │
      │       │           └─ 语言模型解码
      │       │               → 生成文本 tokens
      │       │
      │       └─ 返回 RequestOutput promise
      │
      └─ 步骤 7: 返回响应（与纯文本相同）
          ├─ 流式: StreamingResponse(chat_stream_generator())
          └─ 非流式: JSONResponse(ChatCompletionResponse)

  关键数据结构（多模态）

  # 1. 解析阶段
  MultimodalData = {
      "modality": "image",
      "data": <Coroutine: load_image_async>
  }

  # 2. 追踪器状态
  MultimodalDataTracker._data = {
      "image": [<Coroutine: load_image_async>]
  }
  MultimodalDataTracker._placeholder_counts = {
      "<image>": 1
  }

  # 3. 加载后的数据
  mm_data = {
      "image": [torch.Tensor([3, 384, 384])]  # 已加载的图像张量
  }

  # 4. 最终输入
  TextPrompt = {
      "prompt": "User: <image>描述这张图\nAssistant: ",
      "multi_modal_data": {
          "image": [torch.Tensor([3, 384, 384])]
      }
  }

  # 5. 处理后的多模态参数
  MultimodalParams = {
      "multimodal_input": MultimodalInput(
          multimodal_hashes=[[int32, int32, ...]],  # Blake3 hash
          multimodal_positions=[3],  # <image> token 在序列中的位置
          multimodal_lengths=[576]    # 图像占用 576 个 tokens
      ),
      "multimodal_data": {
          "image": {
              "pixel_values": torch.Tensor([1, 3, 384, 384]),
              "image_height": 384,
              "image_width": 384
          }
      }
  }

  ---

● 三、关键差异对比表

  | 处理阶段               | DeepSeek-R1（纯文本）                                              | DeepSeek-VL（多模态）
                                                         | 核心差异                        |
  |--------------------|---------------------------------------------------------------|---------------------------------------------
  -----------------------------------------|-----------------------------|
  | 1. 消息解析            | parse_chat_message_content()→ 仅提取文本字符串                        | parse_chat_message_content()→
  识别 image_url 类型→ 创建异步加载器                               | ⭐ 多模态会创建 Coroutine 对象而非直接加载 |
  | 2. 数据追踪            | MultimodalDataTracker→ _data = {}→ _placeholder_counts = {}   | MultimodalDataTracker→ _data = {"image":
   [coroutine]}→ _placeholder_counts = {"": 1} | ⭐ 多模态追踪异步加载任务和占位符           |
  | 3. 占位符处理           | add_multimodal_placeholders()→ 直接返回原文本                        | add_multimodal_placeholders()→
  在文本中插入 <image>                                        | ⭐ 多模态修改文本内容                 |
  | 4. 聊天模板            | "User: 你好\nAssistant: "                                       | "User: 描述这张图\nAssistant: "
                                                      | ⭐ 多模态模板包含占位符 token          |
  | 5. 异步加载            | mm_data = await None→ 跳过                                      | mm_data = await mm_coroutines→
  并发下载/加载图像→ 转换为 torch.Tensor                           | ⭐⭐ 多模态实际加载媒体数据              |
  | 6. 输入格式            | TextPrompt({&nbsp;&nbsp;"prompt": "..."})                     | TextPrompt({&nbsp;&nbsp;"prompt":
  "...",&nbsp;&nbsp;"multi_modal_data": {...}})      | ⭐ 多模态添加 multi_modal_data 字段 |
  | 7. Input Processor | DefaultInputProcessor→ tokenize(prompt)→ 返回 (token_ids, None) | BaseMultimodalInputProcessor→ 处理占位符→
  视觉预处理→ 返回 (token_ids, MultimodalParams)         | ⭐⭐⭐ 多模态需要复杂的预处理             |
  | 8. Executor 提交     | submit(token_ids)                                             | submit(token_ids, multimodal_params)
                                             | ⭐ 多模态额外传递视觉参数               |
  | 9. Engine 推理       | 文本 Embedding→ Transformer 解码                                  | 文本 Embedding+ 视觉编码器→ 嵌入融合→
  Transformer 解码                                            | ⭐⭐⭐ 多模态需要视觉编码和融合            |
  | 10. 后处理            | detokenize(output_ids)                                        | detokenize(output_ids)
                                              | ✓ 完全相同                      |

  ---
  四、代码级别的关键分支点

  分支点 1：消息内容解析（chat_utils.py:80-145）

  def parse_chat_message_content_part(part, mm_data_tracker):
      """关键分支：根据 part 类型走不同路径"""

      if isinstance(part, str):
          return part  # 纯文本路径 ✓

      part_type = part.get("type")

      if part_type == "text":
          return cast(str, content)  # 纯文本路径 ✓

      elif part_type == "image_url":  # ← DeepSeek-VL 走这里
          async def load_image_async():
              image_kwargs = multimodal_server_config.media_io_kwargs.get("image", {})
              return await async_load_image(str_content, **image_kwargs)
          return MultimodalData(modality="image", data=load_image_async())

      elif part_type == "video_url":
          # 视频模型路径
          ...

      elif part_type == "audio_url":
          # 音频模型路径
          ...

  分支决定因素： request.messages[i].content[j].type

  ---
  分支点 2：输入处理器选择（inputs/registry.py）

  def create_input_processor(model_dir, tokenizer, checkpoint_format):
      """根据模型类型自动选择处理器"""

      model_config = AutoConfig.from_pretrained(model_dir)
      model_type = model_config.model_type

      # 查找注册的处理器
      if model_type in MULTIMODAL_INPUT_PROCESSOR_REGISTRY:
          # DeepSeek-VL 等多模态模型走这里 ⭐
          processor_cls = MULTIMODAL_INPUT_PROCESSOR_REGISTRY[model_type]
          return processor_cls(model_config, tokenizer)
      else:
          # DeepSeek-R1 等纯文本模型走这里 ✓
          return DefaultInputProcessor(tokenizer)

  分支决定因素： model_config.model_type（在模型 config.json 中定义）

  ---
  分支点 3：多模态数据加载（openai_server.py:521-523）

  async def openai_chat(request, raw_request):
      # ... 前面的处理 ...

      mm_data = await mm_coroutines

      if mm_data is not None:  # ← 关键分支
          # 多模态路径：添加数据 ⭐
          prompt["multi_modal_data"] = mm_data
      else:
          # 纯文本路径：不添加字段 ✓
          pass

  分支决定因素： mm_coroutines 是否为 None（由消息中是否包含多模态内容决定）

  ---
  五、性能和资源差异

  | 指标         | 纯文本模型         | 多模态模型                | 影响                  |
  |------------|---------------|----------------------|---------------------|
  | 网络 I/O     | 无             | 异步下载图像/视频            | +200ms ~ +2s（取决于网络） |
  | 内存占用       | ~1-2 GB（模型权重） | ~3-5 GB（模型权重 + 图像缓存） | +1-3 GB             |
  | 计算量        | 仅 Transformer | Transformer + 视觉编码器  | +30% ~ +100% 计算     |
  | 首 Token 延迟 | ~100-500ms    | ~300-1500ms          | +200-1000ms         |
  | 吞吐量        | ~100 tokens/s | ~60-80 tokens/s      | -20% ~ -40%         |

  ---
  六、特殊场景：MultimodalEncoder 服务器

  对于分离式架构（disaggregated serving），TensorRT-LLM 支持独立的视觉编码器服务：

  # 初始化为 MM_ENCODER 角色
  server = OpenAIServer(
      llm=MultimodalEncoder(model_path),
      server_role=ServerRole.MM_ENCODER  # ← 特殊角色
  )

  # 注册不同的路由
  server.register_mm_encoder_routes()
      ↓
  # 仅注册 /v1/chat/completions → openai_mm_encoder()

  MultimodalEncoder 调用链

  POST /v1/chat/completions
      ↓
  openai_mm_encoder(request)  # openai_server.py:568
      ├─ 解析消息（与正常流程相同）
      ├─ 应用聊天模板
      ├─ await mm_coroutines（加载图像）
      ├─ llm.generate_async(inputs=prompt)  # llm 是 MultimodalEncoder
      │   └─ MultimodalEncoder.generate_async()  # mm_encoder.py:129
      │       └─ 仅执行视觉编码，不生成文本
      │           → 返回 mm_embedding_handle
      │
      └─ 返回响应
          └─ ChatCompletionResponse(
                  mm_embedding_handle={  # ← 特殊字段
                      "tensor_size": [576, 2048],
                      "handle": "shared_memory_key"
                  }
              )

  用途： 视觉编码结果可以被其他服务复用，避免重复编码相同图像。

  ---

● 七、总结：统一架构的设计精髓

  TensorRT-LLM 通过以下设计实现"一个服务端适配多种模态"：

  1. 分层抽象原则

  API 层（OpenAI 兼容）
      ↓ 统一接口
  解析层（parse_chat_messages_coroutines）
      ↓ 模态感知
  追踪层（MultimodalDataTracker）
      ↓ 异步加载
  处理层（BaseMultimodalInputProcessor）← 模型特定
      ↓ 标准化输出
  推理层（GenerationExecutor）
      ↓ 通用引擎

  2. 关键设计模式

  模式 1：延迟加载（Lazy Loading）
  # 解析时只创建 Coroutine，不立即加载
  MultimodalData(modality="image", data=load_image_async())

  # 在需要时才并发加载
  mm_data = await asyncio.gather(*all_coroutines)

  模式 2：策略模式（Strategy Pattern）
  # 根据模态类型选择加载策略
  LOADER_MAP = {
      "image": async_load_image,
      "video": async_load_video,
      "audio": async_load_audio
  }

  模式 3：注册表模式（Registry Pattern）
  # 模型特定处理器注册
  MULTIMODAL_INPUT_PROCESSOR_REGISTRY = {
      "deepseek_vl": DeepSeekVLInputProcessor,
      "qwen2_vl": Qwen2VLInputProcessor,
      "llava": LLaVAInputProcessor,
      ...
  }

  3. 数据流对比图

  ┌─────────────────────────────────────────────────┐
  │                   用户请求                        │
  └─────────────────────────────────────────────────┘
                        ↓
          ┌─────────────┴─────────────┐
          │                           │
     【纯文本】                    【多模态】
          │                           │
          ↓                           ↓
      文本解析                   文本 + 媒体解析
          │                           │
          ↓                           ↓
      聊天模板                   聊天模板 + 占位符
          │                           │
          ↓                           ↓
      跳过加载                   异步加载媒体
          │                           │
          ↓                           ↓
     TextPrompt              TextPrompt + mm_data
          │                           │
          └─────────────┬─────────────┘
                        ↓
                Input Processor
                        │
          ┌─────────────┴─────────────┐
          │                           │
     DefaultProcessor        MultimodalProcessor
     (token_ids, None)      (token_ids, MultimodalParams)
          │                           │
          └─────────────┬─────────────┘
                        ↓
                   TRT Executor
                        │
          ┌─────────────┴─────────────┐
          │                           │
      纯文本推理              多模态推理（视觉 + 文本）
          │                           │
          └─────────────┬─────────────┘
                        ↓
                    统一输出
                        ↓
                ChatCompletionResponse

  ---
  八、实战建议

  如果你要基于此架构开发或调试：

  调试纯文本模型（DeepSeek-R1）

  1. 关注点：parse_chat_message_content() 是否正确提取文本
  2. 验证：mm_coroutines 应该为 None
  3. 检查：prompt 字典不应包含 multi_modal_data 字段

  调试多模态模型（DeepSeek-VL）

  1. 关注点 1：parse_chat_message_content_part() 是否识别 image_url
  2. 关注点 2：MultimodalDataTracker._data 是否包含 coroutine
  3. 关注点 3：await mm_coroutines 是否成功加载图像（检查网络）
  4. 关注点 4：multi_modal_data 格式是否符合模型预期
  5. 验证：打印 MultimodalParams 查看占位符位置是否正确

  性能优化建议

  1. 图像缓存：对相同 URL 的图像进行缓存
  2. 预加载：提前下载常用图像
  3. 批处理：合并多个请求的视觉编码
  4. 分离式架构：使用 MultimodalEncoder 独立服务

  ---
  九、关键文件速查表

  | 文件路径                   | 核心职责       | 关键函数/类                                                 |
  |------------------------|------------|--------------------------------------------------------|
  | serve/openai_server.py | 服务器入口、路由   | openai_chat(), openai_mm_encoder()                     |
  | serve/chat_utils.py    | 消息解析、模态识别  | parse_chat_messages_coroutines()                       |
  | inputs/utils.py        | 多模态数据追踪、加载 | MultimodalDataTracker, async_load_image()              |
  | inputs/registry.py     | 处理器注册、选择   | create_input_processor(), BaseMultimodalInputProcessor |
  | inputs/data.py         | 数据结构定义     | TextPrompt, TokensPrompt                               |
  | inputs/multimodal.py   | 多模态参数      | MultimodalParams, MultimodalInput                      |
  | llmapi/llm.py          | LLM 核心逻辑   | generate_async()                                       |
  | llmapi/mm_encoder.py   | 视觉编码器      | MultimodalEncoder                                      |

  ---
  总结： TensorRT-LLM 通过 统一的 API 层 + 模态感知的解析层 + 可扩展的处理器层 实现了一套代码同时支持纯文本和多模态模型。核心思想是：
  1. 早期分流：在解析阶段识别模态类型
  2. 延迟加载：异步处理媒体数据，不阻塞主流程
  3. 策略模式：根据模型类型动态选择处理器
  4. 统一输出：所有模型最终产生相同格式的 RequestOutput
